{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e3231f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from ark_nlp.model.ner.global_pointer_bert import GlobalPointerBert\n",
    "from ark_nlp.model.ner.global_pointer_bert import GlobalPointerBertConfig\n",
    "from ark_nlp.model.ner.global_pointer_bert import Dataset\n",
    "from ark_nlp.model.ner.global_pointer_bert import Task\n",
    "from ark_nlp.model.ner.global_pointer_bert import get_default_model_optimizer\n",
    "from ark_nlp.model.ner.global_pointer_bert import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445733e0",
   "metadata": {},
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151b330",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c75b6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ark_nlp.factory.utils.conlleval import get_entity_bio\n",
    "\n",
    "\n",
    "datalist = []\n",
    "with open('../data/train_data/train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines.append('\\n')\n",
    "    \n",
    "    text = []\n",
    "    labels = []\n",
    "    label_set = set()\n",
    "    \n",
    "    for line in lines: \n",
    "        if line == '\\n':                \n",
    "            text = ''.join(text)\n",
    "            entity_labels = []\n",
    "            for _type, _start_idx, _end_idx in get_entity_bio(labels, id2label=None):\n",
    "                entity_labels.append({\n",
    "                    'start_idx': _start_idx,\n",
    "                    'end_idx': _end_idx,\n",
    "                    'type': _type,\n",
    "                    'entity': text[_start_idx: _end_idx+1]\n",
    "                })\n",
    "                \n",
    "            if text == '':\n",
    "                continue\n",
    "            \n",
    "            datalist.append({\n",
    "                'text': text,\n",
    "                'label': entity_labels\n",
    "            })\n",
    "            \n",
    "            text = []\n",
    "            labels = []\n",
    "            \n",
    "        elif line == '  O\\n':\n",
    "            text.append(' ')\n",
    "            labels.append('O')\n",
    "        else:\n",
    "            line = line.strip('\\n').split()\n",
    "            if len(line) == 1:\n",
    "                term = ' '\n",
    "                label = line[0]\n",
    "            else:\n",
    "                term, label = line\n",
    "            text.append(term)\n",
    "            label_set.add(label.split('-')[-1])\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fb222b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里随意分割了一下看指标，建议实际使用sklearn分割或者交叉验证\n",
    "# train_data_df = pd.DataFrame(datalist)\n",
    "# train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "# dev_data_df = pd.DataFrame(datalist[-400:])\n",
    "# dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_df = pd.DataFrame(datalist)\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "train_data_df, dev_data_df = train_test_split(train_data_df, test_size=0.1, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5178f86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start_idx': 0, 'end_idx': 1, 'type': '40', 'entity': '手机'},\n",
       " {'start_idx': 2, 'end_idx': 4, 'type': '4', 'entity': '三脚架'},\n",
       " {'start_idx': 5, 'end_idx': 6, 'type': '14', 'entity': '网红'},\n",
       " {'start_idx': 7, 'end_idx': 8, 'type': '5', 'entity': '直播'},\n",
       " {'start_idx': 9, 'end_idx': 10, 'type': '4', 'entity': '支架'},\n",
       " {'start_idx': 11, 'end_idx': 12, 'type': '7', 'entity': '桌面'},\n",
       " {'start_idx': 13, 'end_idx': 15, 'type': '4', 'entity': '自拍杆'},\n",
       " {'start_idx': 16, 'end_idx': 17, 'type': '11', 'entity': '蓝牙'},\n",
       " {'start_idx': 18, 'end_idx': 19, 'type': '11', 'entity': '遥控'},\n",
       " {'start_idx': 20, 'end_idx': 22, 'type': '4', 'entity': '三脚架'},\n",
       " {'start_idx': 23, 'end_idx': 24, 'type': '5', 'entity': '摄影'},\n",
       " {'start_idx': 25, 'end_idx': 26, 'type': '5', 'entity': '拍摄'},\n",
       " {'start_idx': 27, 'end_idx': 28, 'type': '5', 'entity': '拍照'},\n",
       " {'start_idx': 29, 'end_idx': 30, 'type': '13', 'entity': '抖音'},\n",
       " {'start_idx': 31, 'end_idx': 35, 'type': '4', 'entity': '看电视神器'},\n",
       " {'start_idx': 36, 'end_idx': 38, 'type': '4', 'entity': '三角架'},\n",
       " {'start_idx': 39, 'end_idx': 40, 'type': '11', 'entity': '便携'},\n",
       " {'start_idx': 41, 'end_idx': 42, 'type': '11', 'entity': '伸缩'},\n",
       " {'start_idx': 43, 'end_idx': 44, 'type': '8', 'entity': '懒人'},\n",
       " {'start_idx': 45, 'end_idx': 46, 'type': '7', 'entity': '户外'},\n",
       " {'start_idx': 47, 'end_idx': 49, 'type': '4', 'entity': '支撑架'},\n",
       " {'start_idx': 52, 'end_idx': 54, 'type': '16', 'entity': '女神粉'},\n",
       " {'start_idx': 58, 'end_idx': 60, 'type': '4', 'entity': '三脚架'},\n",
       " {'start_idx': 62, 'end_idx': 63, 'type': '11', 'entity': '蓝牙'},\n",
       " {'start_idx': 64, 'end_idx': 65, 'type': '11', 'entity': '遥控'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(train_data_df['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13af45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df, categories=label_set)\n",
    "ner_dev_dataset = Dataset(dev_data_df, categories=ner_train_dataset.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806a983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../pretrain_model/chinese-roberta-wwm-ext-large/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1e13c",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b67a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=path, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc70e1f2",
   "metadata": {},
   "source": [
    "#### 3. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec30143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer)\n",
    "ner_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0b9cd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61d6d7",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40452dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GlobalPointerBertConfig.from_pretrained(path, num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee1990",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeb526e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd7c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrain_model/chinese-roberta-wwm-ext-large/ were not used when initializing GlobalPointerBert: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing GlobalPointerBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GlobalPointerBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GlobalPointerBert were not initialized from the model checkpoint at ../pretrain_model/chinese-roberta-wwm-ext-large/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'global_pointer.dense.bias', 'global_pointer.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dl_module = GlobalPointerBert.from_pretrained(path, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33926bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03067a22",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4be00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31fd068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_model_optimizer(dl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a69ba0",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13bce8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'gpce', cuda_device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae4963",
   "metadata": {},
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63ac055c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████▍                                             | 100/1125 [00:42<07:24,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/1125],train loss is:2.875331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████▉                                         | 200/1125 [01:24<06:26,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/1125],train loss is:1.671295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████▎                                    | 300/1125 [02:06<05:45,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/1125],train loss is:1.238539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████▊                                | 400/1125 [02:49<05:03,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/1125],train loss is:1.013926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████▏                           | 500/1125 [03:30<04:19,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/1125],train loss is:0.875486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████▋                       | 600/1125 [04:12<03:37,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/1125],train loss is:0.780941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████                   | 700/1125 [04:53<02:56,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699/1125],train loss is:0.713699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████▌              | 800/1125 [05:35<02:14,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[799/1125],train loss is:0.662399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████          | 900/1125 [06:17<01:33,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[899/1125],train loss is:0.621719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████▌     | 1000/1125 [06:58<00:51,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999/1125],train loss is:0.588364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████▉ | 1100/1125 [07:40<00:10,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1099/1125],train loss is:0.561435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1125/1125 [07:50<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0],train loss is:0.555345 \n",
      "\n",
      "eval loss is 0.281849, precision is:50834.0, recall is:129103.0, f1_score is:0.7874952557260482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████▍                                             | 100/1125 [00:41<07:07,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/1125],train loss is:0.271461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████▉                                         | 200/1125 [01:22<06:26,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/1125],train loss is:0.272167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████▎                                    | 300/1125 [02:04<05:43,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/1125],train loss is:0.269575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████▊                                | 400/1125 [02:46<05:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/1125],train loss is:0.268301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████▏                           | 500/1125 [03:27<04:19,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/1125],train loss is:0.268452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████▋                       | 600/1125 [04:09<03:38,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/1125],train loss is:0.267422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████                   | 700/1125 [04:50<02:56,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699/1125],train loss is:0.266638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████▌              | 800/1125 [05:32<02:15,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[799/1125],train loss is:0.265916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████          | 900/1125 [06:13<01:33,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[899/1125],train loss is:0.265215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████▌     | 1000/1125 [06:55<00:51,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999/1125],train loss is:0.265080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████▉ | 1100/1125 [07:36<00:10,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1099/1125],train loss is:0.264745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1125/1125 [07:47<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[1],train loss is:0.264626 \n",
      "\n",
      "eval loss is 0.266450, precision is:51339.0, recall is:129155.0, f1_score is:0.7949982579071658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████▍                                             | 100/1125 [00:41<07:03,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/1125],train loss is:0.238633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████▉                                         | 200/1125 [01:22<06:25,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/1125],train loss is:0.238633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████▎                                    | 300/1125 [02:05<05:54,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/1125],train loss is:0.238617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████▊                                | 400/1125 [02:47<05:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/1125],train loss is:0.238723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████▏                           | 500/1125 [03:29<04:21,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/1125],train loss is:0.239092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████▋                       | 600/1125 [04:11<03:37,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/1125],train loss is:0.238997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████                   | 700/1125 [04:52<02:56,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699/1125],train loss is:0.238403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████▌              | 800/1125 [05:34<02:14,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[799/1125],train loss is:0.239020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████          | 900/1125 [06:15<01:33,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[899/1125],train loss is:0.238548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████▌     | 1000/1125 [06:57<00:51,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999/1125],train loss is:0.238652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████▉ | 1100/1125 [07:38<00:10,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1099/1125],train loss is:0.238787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1125/1125 [07:48<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[2],train loss is:0.238913 \n",
      "\n",
      "eval loss is 0.263871, precision is:52545.0, recall is:130746.0, f1_score is:0.8037721995319168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████▍                                             | 100/1125 [00:42<07:17,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/1125],train loss is:0.218775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████▉                                         | 200/1125 [01:25<06:36,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/1125],train loss is:0.216446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████▎                                    | 300/1125 [02:08<05:56,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/1125],train loss is:0.216180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████▊                                | 400/1125 [02:52<05:12,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/1125],train loss is:0.216733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████▏                           | 500/1125 [03:35<04:30,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/1125],train loss is:0.216783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████▋                       | 600/1125 [04:18<03:40,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/1125],train loss is:0.216917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████                   | 700/1125 [05:00<03:02,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699/1125],train loss is:0.217385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████▌              | 800/1125 [05:43<02:21,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[799/1125],train loss is:0.217770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████          | 900/1125 [06:25<01:33,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[899/1125],train loss is:0.218223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████▌     | 1000/1125 [07:06<00:51,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999/1125],train loss is:0.218093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████▉ | 1100/1125 [07:48<00:10,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1099/1125],train loss is:0.217982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1125/1125 [07:58<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[3],train loss is:0.218072 \n",
      "\n",
      "eval loss is 0.266369, precision is:51717.0, recall is:129128.0, f1_score is:0.8010191437953039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████▍                                             | 100/1125 [00:41<07:04,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/1125],train loss is:0.192180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████▉                                         | 200/1125 [01:22<06:23,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/1125],train loss is:0.194129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████▎                                    | 300/1125 [02:06<05:55,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/1125],train loss is:0.194167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████████▊                                | 400/1125 [02:49<05:13,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/1125],train loss is:0.195416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████▏                           | 500/1125 [03:32<04:18,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/1125],train loss is:0.196097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████▋                       | 600/1125 [04:13<03:38,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/1125],train loss is:0.196861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████                   | 700/1125 [04:55<02:57,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699/1125],train loss is:0.196512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████▌              | 800/1125 [05:37<02:14,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[799/1125],train loss is:0.196663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████          | 900/1125 [06:19<01:33,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[899/1125],train loss is:0.197268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|███████████████████████████████████████████▌     | 1000/1125 [07:00<00:52,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999/1125],train loss is:0.197640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|███████████████████████████████████████████████▉ | 1100/1125 [07:43<00:10,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1099/1125],train loss is:0.197836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1125/1125 [07:54<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[4],train loss is:0.197796 \n",
      "\n",
      "eval loss is 0.283281, precision is:53493.0, recall is:133319.0, f1_score is:0.8024812667361741\n"
     ]
    }
   ],
   "source": [
    "model.fit(ner_train_dataset, \n",
    "          ner_dev_dataset,\n",
    "          lr=2e-5,\n",
    "          epochs=num_epoches, \n",
    "          batch_size=batch_size\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd276eeb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、生成提交数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d8a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ark-nlp提供该函数：from ark_nlp.model.ner.global_pointer_bert import Predictor\n",
    "# 这里主要是为了可以比较清晰地看到解码过程，所以将代码copy到这\n",
    "class GlobalPointerNERPredictor(object):\n",
    "    \"\"\"\n",
    "    GlobalPointer命名实体识别的预测器\n",
    "\n",
    "    Args:\n",
    "        module: 深度学习模型\n",
    "        tokernizer: 分词器\n",
    "        cat2id (:obj:`dict`): 标签映射\n",
    "    \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        module,\n",
    "        tokernizer,\n",
    "        cat2id\n",
    "    ):\n",
    "        self.module = module\n",
    "        self.module.task = 'TokenLevel'\n",
    "\n",
    "        self.cat2id = cat2id\n",
    "        self.tokenizer = tokernizer\n",
    "        self.device = list(self.module.parameters())[0].device\n",
    "\n",
    "        self.id2cat = {}\n",
    "        for cat_, idx_ in self.cat2id.items():\n",
    "            self.id2cat[idx_] = cat_\n",
    "\n",
    "    def _convert_to_transfomer_ids(\n",
    "        self,\n",
    "        text\n",
    "    ):\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        token_mapping = self.tokenizer.get_token_mapping(text, tokens)\n",
    "\n",
    "        input_ids = self.tokenizer.sequence_to_ids(tokens)\n",
    "        input_ids, input_mask, segment_ids = input_ids\n",
    "\n",
    "        zero = [0 for i in range(self.tokenizer.max_seq_len)]\n",
    "        span_mask = [input_mask for i in range(sum(input_mask))]\n",
    "        span_mask.extend([zero for i in range(sum(input_mask), self.tokenizer.max_seq_len)])\n",
    "        span_mask = np.array(span_mask)\n",
    "\n",
    "        features = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': input_mask,\n",
    "            'token_type_ids': segment_ids,\n",
    "            'span_mask': span_mask\n",
    "        }\n",
    "\n",
    "        return features, token_mapping\n",
    "\n",
    "    def _get_input_ids(\n",
    "        self,\n",
    "        text\n",
    "    ):\n",
    "        if self.tokenizer.tokenizer_type == 'vanilla':\n",
    "            return self._convert_to_vanilla_ids(text)\n",
    "        elif self.tokenizer.tokenizer_type == 'transfomer':\n",
    "            return self._convert_to_transfomer_ids(text)\n",
    "        elif self.tokenizer.tokenizer_type == 'customized':\n",
    "            return self._convert_to_customized_ids(text)\n",
    "        else:\n",
    "            raise ValueError(\"The tokenizer type does not exist\")\n",
    "\n",
    "    def _get_module_one_sample_inputs(\n",
    "        self,\n",
    "        features\n",
    "    ):\n",
    "        return {col: torch.Tensor(features[col]).type(torch.long).unsqueeze(0).to(self.device) for col in features}\n",
    "\n",
    "    def predict_one_sample(\n",
    "        self,\n",
    "        text='',\n",
    "        threshold=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        单样本预测\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`string`): 输入文本\n",
    "            threshold (:obj:`float`, optional, defaults to 0): 预测的阈值\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        features, token_mapping = self._get_input_ids(text)\n",
    "        self.module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = self._get_module_one_sample_inputs(features)\n",
    "            scores = self.module(**inputs)[0].cpu()\n",
    "            \n",
    "        scores[:, [0, -1]] -= np.inf\n",
    "        scores[:, :, [0, -1]] -= np.inf\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for category, start, end in zip(*np.where(scores > threshold)):\n",
    "            if end-1 > token_mapping[-1][-1]:\n",
    "                break\n",
    "            if token_mapping[start-1][0] <= token_mapping[end-1][-1]:\n",
    "                entitie_ = {\n",
    "                    \"start_idx\": token_mapping[start-1][0],\n",
    "                    \"end_idx\": token_mapping[end-1][-1],\n",
    "                    \"entity\": text[token_mapping[start-1][0]: token_mapping[end-1][-1]+1],\n",
    "                    \"type\": self.id2cat[category]\n",
    "                }\n",
    "\n",
    "                if entitie_['entity'] == '':\n",
    "                    continue\n",
    "\n",
    "                entities.append(entitie_)\n",
    "\n",
    "        return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "614f38d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = GlobalPointerNERPredictor(model.module, tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c6c18cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 10000/10000 [06:09<00:00, 27.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predict_results = []\n",
    "\n",
    "with open('../data/preliminary_test_a/sample_per_line_preliminary_A.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for _line in tqdm(lines):\n",
    "        label = len(_line) * ['O']\n",
    "        for _preditc in ner_predictor_instance.predict_one_sample(_line[:-1]):\n",
    "            if 'I' in label[_preditc['start_idx']]:\n",
    "                continue\n",
    "            if 'B' in label[_preditc['start_idx']] and 'O' not in label[_preditc['end_idx']]:\n",
    "                continue\n",
    "            if 'O' in label[_preditc['start_idx']] and 'B' in label[_preditc['end_idx']]:\n",
    "                continue\n",
    "\n",
    "            label[_preditc['start_idx']] = 'B-' + _preditc['type']\n",
    "            label[_preditc['start_idx']+1: _preditc['end_idx']+1] = (\n",
    "                _preditc['end_idx'] - _preditc['start_idx']) * [('I-' + _preditc['type'])]\n",
    "\n",
    "        predict_results.append([_line, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77a8ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gobal_pointer_baseline1.txt', 'w', encoding='utf-8') as f:\n",
    "    for _result in predict_results:\n",
    "        for word, tag in zip(_result[0], _result[1]):\n",
    "            if word == '\\n':\n",
    "                continue\n",
    "            f.write(f'{word} {tag}\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9359365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = set()\n",
    "s.add(2)\n",
    "s.add(3)\n",
    "s.add(1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a90de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "del s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b02b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
